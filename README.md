# Olist Seller Churn Prediction

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/package%20manager-uv-purple.svg)](https://github.com/astral-sh/uv)
[![Docker](https://img.shields.io/badge/container-Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

End-to-end machine learning pipeline to predict seller churn for the Olist e-commerce marketplace â€” enabling proactive retention strategies and measurable revenue protection.

---

## ğŸ¯ Business Problem

Olist's marketplace depends on an active seller base. High churn erodes GMV and increases acquisition costs. This project builds a **dual-stage predictive system** that flags at-risk sellers before they churn, giving account managers a prioritised intervention list with estimated revenue impact.

### Key Results (842 sellers, Jun 2017 â€“ Aug 2018)

| Metric | Value |
|--------|-------|
| Overall Churn Rate | **85.0%** |
| Never-Activated Sellers | **515 (61.2%)** â€” onboarding failure |
| Dormant Sellers | **201 (23.9%)** â€” retention failure |
| Active Sellers | **140 (16.6%)** |
| Revenue at Risk | **R$ 272,607** |
| High / Critical Risk Sellers | **669 (79.5%)** |
| Pre-Activation Model AUC | **0.975** (Logistic Regression) |
| Retention Model AUC | **0.706** (Gradient Boosting) |
| Active Sellers Targeted for Intervention | **33** |

---

## ğŸ—ï¸ Architecture

```
Raw CSVs
   â”‚
   â–¼
DataLoader â”€â”€â–º DataPreprocessor â”€â”€â–º ChurnAnalyzer (labels + cohorts)
                                          â”‚
                                          â–¼
                                  FeatureEngineer
                                   /            \
                        Pre-Activation         Retention
                          Features              Features
                              â”‚                    â”‚
                              â–¼                    â–¼
                          ChurnModeler â”€â”€â”€â”€â”€â”€â–º ChurnModeler
                       (LogReg / RF / GBM)  (LogReg / RF / GBM)
                              â”‚                    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â–¼
                                 ModelEvaluator
                              (ROC Â· PR Â· CM Â· FI)
                                       â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼                         â–¼
                   Risk Scoring              InsightsReporter
                (overall_churn_risk)    (churn_insights_report.md)
                          â”‚
                          â–¼
               InterventionPrioritizer
              (intervention_priority_list.csv)
                          â”‚
                          â–¼
               ğŸ“Š Interactive Dashboard
              (dashboard/index.html)
```

---

## ğŸš€ Quick Start

### Option A â€” Local (uv)

```bash
# 1. Clone and enter the project
git clone <repo>
cd olist-ecommerce

# 2. Copy and configure environment variables
cp .env.example .env   # set DATA_PATH to your Olist CSV folder

# 3. Install dependencies (uv required)
make install

# 4. Run the full pipeline
make run-pipeline

# 5. Generate the interactive dashboard
make dashboard         # â†’ opens dashboard/index.html

# 6. View generated reports
ls outputs/            # seller_master, risk_scores, segments, cohorts
ls outputs/figures/    # ROC, PR, confusion matrix, feature importance
```

### Option B â€” Docker

```bash
# 1. Clone and configure env (same as above)
git clone <repo>
cd olist-ecommerce
cp .env.example .env

# 2. Build the image (one-time, ~2 min)
make docker-build

# 3. Run the full training pipeline
make docker-pipeline

# 4. Launch the MLflow UI â†’ http://localhost:5000
make docker-mlflow
```

> Raw CSVs are read from `./data/raw/` on your host via a bind mount â€” no copying into the image required.

---

## ğŸ³ Docker

The project ships a **multi-stage `Dockerfile`** and a **`docker-compose.yml`** that orchestrates three independent services.

### How it works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  builder stage (python:3.10-slim + build-essential)      â”‚
â”‚   â””â”€ uv sync â†’ resolves & installs deps into .venv       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  COPY .venv only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  runtime stage (python:3.10-slim, no compiler tools)     â”‚
â”‚   â”œâ”€ runs as non-root user (appuser)                     â”‚
â”‚   â”œâ”€ bind mount: ./data  â†’ /app/data  (host CSVs)        â”‚
â”‚   â””â”€ named volumes: outputs Â· models Â· mlruns Â· logs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Services

| Service | Profile | Description |
|---------|---------|-------------|
| `pipeline` | `pipeline` | Runs the full training pipeline |
| `inference` | `inference` | Scores sellers with saved models |
| `mlflow` | `mlflow` | Experiment tracking UI on port 5000 |

Services use **Compose profiles** â€” nothing starts by default. Activate the one you need.

### Commands

```bash
make docker-build      # Build the image (needed once, or after dep changes)
make docker-pipeline   # Train â€” reads ./data/raw, writes to volumes
make docker-inference  # Score â€” uses saved models from the models volume
make docker-mlflow     # Start MLflow UI â†’ http://localhost:5000
make docker-down       # Stop & remove containers
make docker-clean      # âš  Remove containers, volumes AND the image
```

### When to rebuild

You only need `make docker-build` after:
- Changing `Dockerfile`
- Updating `pyproject.toml` or `uv.lock` (dependency changes)
- Modifying files in `src/`, `scripts/`, or `config/`

Changes to `docker-compose.yml` **never** require a rebuild.

---

## ğŸ“Š Interactive Dashboard

After running the pipeline, generate a self-contained stakeholder dashboard:

```bash
make dashboard
# â†’ dashboard/index.html
```

The dashboard is a **single HTML file** (no server required) with five tabs:

| Tab | Contents |
|-----|----------|
| ğŸ  **Overview** | 6 KPI cards Â· key insight pills Â· status & risk donuts |
| ğŸ“… **Cohort Analysis** | Monthly churn vs activation trend Â· GMV by cohort |
| ğŸ—‚ï¸ **Segmentation** | Churn by business segment Â· lead type Â· behaviour profile Â· state |
| ğŸ¤– **Model Performance** | Metric bars per model Â· cross-model comparison chart Â· pipeline architecture |
| ğŸ¯ **Interventions** | Searchable & filterable priority table for 30 high-risk sellers |

Open `dashboard/index.html` in any browser â€” no dependencies, no server.

---

## ğŸ“ Project Structure

```
olist-ecommerce/
â”œâ”€â”€ ğŸ“ config/
â”‚   â””â”€â”€ settings.py              # Centralised config via pydantic-settings (.env)
â”œâ”€â”€ ğŸ“ src/                      # Core library â€” importable modules
â”‚   â”œâ”€â”€ pipeline.py              # End-to-end orchestrator + all pipeline classes
â”‚   â”œâ”€â”€ features.py              # Feature engineering (pre-activation + retention)
â”‚   â”œâ”€â”€ models.py                # Model training (LogReg, RF, GBM comparison)
â”‚   â”œâ”€â”€ evaluation.py            # Evaluation + chart generation â†’ outputs/figures/
â”‚   â”œâ”€â”€ reports.py               # Stakeholder Markdown report generation
â”‚   â””â”€â”€ validation/
â”‚       â””â”€â”€ schemas.py           # Pydantic data validation schemas
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py          # Entry point: runs src/pipeline.main()
â”‚   â””â”€â”€ generate_dashboard.py    # Reads outputs/ CSVs â†’ dashboard/index.html
â”œâ”€â”€ ğŸ“ dashboard/                # Dashboard source (index.html is gitignored)
â”‚   â””â”€â”€ template.html            # HTML/JS/CSS template (Chart.js, dark theme)
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â””â”€â”€ poc_churn_analysis.ipynb # Exploratory analysis
â”œâ”€â”€ ğŸ“ tests/
â”‚   â””â”€â”€ test_pipeline.py         # Unit tests
â”œâ”€â”€ ğŸ“ docs/                     # Project documentation
â”‚   â”œâ”€â”€ dataset.md               # Dataset description
â”‚   â”œâ”€â”€ glossary.md              # Domain glossary
â”‚   â”œâ”€â”€ project.md               # Architecture & design notes
â”‚   â””â”€â”€ poc.md                   # POC findings
â”œâ”€â”€ ğŸ“ outputs/                  # â† Generated on pipeline run (gitignored)
â”‚   â”œâ”€â”€ figures/                 # Charts: ROC, PR, confusion matrix, feature importance
â”‚   â”œâ”€â”€ seller_master.csv        # Full seller dataset with churn labels
â”‚   â”œâ”€â”€ seller_risk_scores.csv   # Per-seller risk scores
â”‚   â”œâ”€â”€ cohort_analysis.csv      # Monthly cohort stats
â”‚   â”œâ”€â”€ segment_analysis_*.csv   # Churn by segment, lead type, state, profile
â”‚   â”œâ”€â”€ intervention_priority_list.csv
â”‚   â””â”€â”€ analysis_summary.txt
â”œâ”€â”€ ğŸ“ models/                   # â† Generated on pipeline run (gitignored)
â”‚   â”œâ”€â”€ pre_activation_model.joblib
â”‚   â””â”€â”€ retention_model.joblib
â”œâ”€â”€ ğŸ“ data/                     # (gitignored) â€” place Olist CSVs here
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ .env.example                 # Required environment variables template
â”œâ”€â”€ Dockerfile                   # Multi-stage image (builder â†’ runtime)
â”œâ”€â”€ docker-compose.yml           # pipeline Â· inference Â· mlflow services
â”œâ”€â”€ .dockerignore                # Keeps build context lean
â”œâ”€â”€ Makefile                     # Developer shortcuts (local + Docker)
â””â”€â”€ requirements.txt             # Pinned dependencies
```

---

## ğŸ¤– Model Details

### Stage 1 â€” Pre-Activation Model
Predicts whether a newly-onboarded seller will **never make a sale** (61% of the dataset).

| Model | AUC-ROC | Accuracy | F1 |
|-------|---------|----------|----|
| **Logistic Regression** âœ… | **0.975** | 0.930 | 0.944 |
| Random Forest | 0.962 | 0.937 | 0.947 |
| Gradient Boosting | 0.953 | 0.918 | 0.933 |

### Stage 2 â€” Retention Model
Predicts whether an **activated seller** will go dormant (60+ days without an order).

| Model | AUC-ROC | Accuracy | F1 |
|-------|---------|----------|----|
| Logistic Regression | 0.647 | 0.597 | 0.638 |
| Random Forest | 0.673 | 0.597 | 0.638 |
| **Gradient Boosting** âœ… | **0.706** | 0.645 | 0.694 |

> Charts for each model (ROC curve, Precision-Recall, Confusion Matrix, Feature Importance) are saved to `reports/figures/` on every run.

---

## ğŸ› ï¸ Developer Commands

### Local

```bash
make install           # Install dependencies via uv
make run-pipeline      # Run the full end-to-end pipeline
make run-inference     # Score sellers using saved models
make dashboard         # Generate dashboard/index.html from latest outputs
make test              # Run unit tests
make test-coverage     # Run tests with HTML coverage report
make lint              # flake8 + black + isort + bandit checks
make format            # Auto-format with black + isort
make ci-check          # Full local CI simulation (lint â†’ typecheck â†’ tests)
make pre-commit-run    # Run all pre-commit hooks over the codebase
make mlflow-ui         # Open MLflow UI at http://localhost:5000
make clean             # Remove __pycache__, .pytest_cache, htmlcov
make clean-outputs     # Remove generated CSVs, reports, and figures
make setup-dirs        # Create required directories from scratch
```

### Docker

```bash
make docker-build      # Build the Docker image
make docker-pipeline   # Run training pipeline in a container
make docker-inference  # Run inference in a container
make docker-mlflow     # Start MLflow UI at http://localhost:5000
make docker-down       # Stop & remove containers
make docker-clean      # âš  Remove containers, volumes AND the image
```

---

## âš™ï¸ Configuration

All settings are managed through `config/settings.py` and read from `.env`.
No hardcoded paths anywhere in the codebase.

```bash
# .env
DATA_PATH=./data/raw          # Olist raw CSV files
OUTPUT_PATH=./outputs         # Generated CSVs and text files
MODELS_PATH=./models          # Saved .joblib models
FIGURES_PATH=./outputs/figures # Charts and plots
```

See `.env.example` for the full list of configurable values.

---

## ğŸ§ª Testing

```bash
make test             # Run all unit tests
make test-coverage    # Run with coverage (open htmlcov/index.html)
```

---

## ğŸ“§ Contact

Cairo Cananea
- Blog: [cairocananea.com.br](https://cairocananea.com.br)
- Linkedin: [Cairo Cananea](https://www.linkedin.com/in/cairocananea/)
- Github: [Cairo Cananea](https://github.com/cacaprog)
